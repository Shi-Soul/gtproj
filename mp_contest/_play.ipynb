{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 15:32:19,539\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-05-24 15:32:26,421\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-05-24 15:32:32.428983: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-24 15:32:32.429072: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-24 15:32:32.429081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "2024-05-24 15:32:37,993\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import ray\n",
    "from torch import nn\n",
    "\n",
    "from typing import *\n",
    "from ray import air\n",
    "from ray import tune\n",
    "from baselines.train.configs import get_experiment_config\n",
    "from ray.rllib.algorithms import ppo\n",
    "from ray.tune import registry\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from baselines.train import make_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp = \"results/torch/pd_matrix/PPO_meltingpot_397b4_00000_0_2024-05-21_13-58-39/checkpoint_007270\"\n",
    "\n",
    "registry.register_env(\"meltingpot\", make_envs.env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trails with the following arguments:  Namespace(num_workers=2, num_gpus=1, local=False, no_tune=False, algo='ppo', framework='torch', exp='pd_matrix', seed=123, results_dir='./results', logging='INFO', wandb=False, downsample=True, as_test=False, continue_training=None, param_sharing=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_cli_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Training Script for Multi-Agent RL in Meltingpot\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Number of workers to use for sample collection. Setting it zero will use same worker for collection and model training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_gpus\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Number of GPUs to run on (can be a fraction)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--local\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If enabled, init ray in local mode.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-tune\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If enabled, no hyper-parameter tuning.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--algo\",\n",
    "        choices=[\"ppo\"],\n",
    "        default=\"ppo\",\n",
    "        help=\"Algorithm to train agents.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--framework\",\n",
    "        choices=[\"tf\", \"torch\"],\n",
    "        default=\"torch\",\n",
    "        help=\"The DL framework specifier (tf2 eager is not supported).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--exp\",\n",
    "        type=str,\n",
    "        choices=[\"pd_matrix\", \"clean_up\"],\n",
    "        default=\"pd_matrix\",\n",
    "        help=\"Name of the substrate to run\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=123,\n",
    "        help=\"Seed to run\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--results_dir\",\n",
    "        type=str,\n",
    "        default=\"./results\",\n",
    "        help=\"Name of the wandb group\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging\",\n",
    "        choices=[\"DEBUG\", \"INFO\", \"WARN\", \"ERROR\"],\n",
    "        default=\"INFO\",\n",
    "        help=\"The level of training and data flow messages to print.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--wandb\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Whether to use WanDB logging.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--downsample\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Whether to downsample substrates in MeltingPot. Defaults to 8.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--as-test\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether this script should be run as a test.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--continue_training\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the checkpoint to continue training from.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--param_sharing\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to share parameters across agents.\",\n",
    "        \n",
    "    )\n",
    "\n",
    "    args = parser.parse_args([\"--num_gpus\",\"1\"])\n",
    "    print(\"Running trails with the following arguments: \", args)\n",
    "    return args\n",
    "\n",
    "args = get_cli_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 15:53:33,584\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2024-05-24 15:53:33,586\tWARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n"
     ]
    }
   ],
   "source": [
    "# del trainer\n",
    "trainer = \"PPO\"\n",
    "default_config = ppo.PPOConfig()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_experiment_config(args, default_config):\n",
    "    from meltingpot import substrate\n",
    "    from ray.rllib.policy import policy\n",
    "    if args.exp == 'pd_matrix':\n",
    "        substrate_name = \"prisoners_dilemma_in_the_matrix__repeated\"\n",
    "    elif args.exp == 'clean_up':\n",
    "        substrate_name = \"clean_up\"\n",
    "    else:\n",
    "        raise Exception(\"Please set --exp to be one of ['pd_matrix', 'clean_up', \\\n",
    "                        ]. Other substrates are not supported.\")\n",
    "\n",
    "    # Fetch player roles\n",
    "    player_roles = substrate.get_config(substrate_name).default_player_roles\n",
    "\n",
    "    if args.downsample:\n",
    "        scale_factor = 8\n",
    "    else:\n",
    "        scale_factor = 1\n",
    "\n",
    "    params_dict = {\n",
    "\n",
    "        # resources\n",
    "        \"num_rollout_workers\": args.num_workers,\n",
    "        \"num_gpus\": args.num_gpus,\n",
    "\n",
    "        # Env\n",
    "        \"env_name\": \"meltingpot\",\n",
    "        \"env_config\": {\"substrate\": substrate_name, \"roles\": player_roles, \"scaled\": scale_factor},\n",
    "\n",
    "\n",
    "        # training\n",
    "        \"seed\": args.seed,\n",
    "        # \"rollout_fragment_length\": 10,\n",
    "        \"train_batch_size\": 8000,\n",
    "        \"sgd_minibatch_size\": 1024,\n",
    "        'num_sgd_iter': 20,\n",
    "        \"lr\": 1e-4,\n",
    "        \"disable_observation_precprocessing\": True,\n",
    "        \"use_new_rl_modules\": False,\n",
    "        \"use_new_learner_api\": False,\n",
    "        \"framework\": args.framework,\n",
    "        \"exploration_config\": {\n",
    "            # \"type\":MyEpsExp,\n",
    "            \"type\":\"EpsilonGreedy\",\n",
    "            \"warmup_timesteps\": 1e5,\n",
    "            \"epsilon_timesteps\": 1e7,\n",
    "            \n",
    "            },\n",
    "        # \"exploration_config\":{\n",
    "            # \"type\": \"MyEpsExp\",\n",
    "            # \"type\":\"RE3\",\n",
    "            # \"sub_exploration\":{\n",
    "            #     \"type\": \"StochasticSampling\",\n",
    "            # }\n",
    "            # # \"type\":\"EpsilonGreedy\",\n",
    "            # # \"warmup_timesteps\": 1e5,\n",
    "            # # \"epsilon_timesteps\": 1e7,\n",
    "            # },\n",
    "\n",
    "        # agent model\n",
    "        \"fcnet_hidden\": (256, 256),\n",
    "        \"post_fcnet_hidden\": (128,),\n",
    "        \"cnn_activation\": \"silu\",\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "        \"post_fcnet_activation\": \"relu\",\n",
    "        \"use_lstm\": True,\n",
    "        \"max_seq_len\": 40,\n",
    "        \"lstm_use_prev_action\": True,\n",
    "        \"lstm_use_prev_reward\": False,\n",
    "        \"lstm_cell_size\": 256,\n",
    "        \"shared_policy\": args.param_sharing,\n",
    "\n",
    "        # experiment trials\n",
    "        \"exp_name\": args.exp,\n",
    "        \"stopping\": {\n",
    "                    # \"timesteps_total\": 1000000,\n",
    "                    # \"training_iteration\": 100,\n",
    "                    \"episode_reward_mean\": 100,\n",
    "        },\n",
    "        \"num_checkpoints\": 5,\n",
    "        \"checkpoint_interval\": 50,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"results_dir\": args.results_dir,\n",
    "        \"logging\": args.logging,\n",
    "\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Preferrable to update the parameters in above dict before changing anything below\n",
    "    \n",
    "    run_configs: ppo.PPOConfig = default_config\n",
    "    experiment_configs = {}\n",
    "    tune_configs = None\n",
    "\n",
    "    # Resources \n",
    "    run_configs.num_rollout_workers = params_dict['num_rollout_workers']\n",
    "    run_configs.num_gpus = params_dict['num_gpus']\n",
    "\n",
    "\n",
    "    # Training\n",
    "    run_configs.train_batch_size = params_dict['train_batch_size']\n",
    "    run_configs.sgd_minibatch_size = params_dict['sgd_minibatch_size']\n",
    "    run_configs.num_sgd_iter = params_dict['num_sgd_iter']\n",
    "    run_configs.lr = params_dict['lr']\n",
    "    run_configs.preprocessor_pref = None\n",
    "    run_configs._disable_preprocessor_api = params_dict['disable_observation_precprocessing']\n",
    "    run_configs.rl_module(_enable_rl_module_api=params_dict['use_new_rl_modules'])\n",
    "    run_configs.training(_enable_learner_api=params_dict['use_new_learner_api'])\n",
    "    run_configs.exploration(exploration_config=params_dict['exploration_config'])\n",
    "    run_configs = run_configs.framework(params_dict['framework'])\n",
    "    run_configs.log_level = params_dict['logging']\n",
    "    run_configs.seed = params_dict['seed']\n",
    "\n",
    "    # Environment\n",
    "    run_configs.env = params_dict['env_name']\n",
    "    run_configs.env_config = params_dict['env_config']\n",
    "\n",
    "    # Setup multi-agent policies. The below code will initialize independent\n",
    "    # policies for each agent.\n",
    "    if not params_dict['shared_policy']:\n",
    "        base_env = make_envs.env_creator(run_configs.env_config)\n",
    "        policies = {}\n",
    "        player_to_agent = {}\n",
    "        for i in range(len(player_roles)):\n",
    "            rgb_shape = base_env.observation_space[f\"player_{i}\"][\"RGB\"].shape\n",
    "            sprite_x = rgb_shape[0]\n",
    "            sprite_y = rgb_shape[1]\n",
    "\n",
    "            policies[f\"agent_{i}\"] = policy.PolicySpec(\n",
    "                observation_space=base_env.observation_space[f\"player_{i}\"],\n",
    "                action_space=base_env.action_space[f\"player_{i}\"],\n",
    "                config={\n",
    "                    \"model\": {\n",
    "                        \"conv_filters\": [[16, [8, 8], 1],\n",
    "                                        [128, [sprite_x, sprite_y], 1]],\n",
    "                    },\n",
    "                })\n",
    "            player_to_agent[f\"player_{i}\"] = f\"agent_{i}\"\n",
    "\n",
    "        run_configs.multi_agent(policies=policies, policy_mapping_fn=(lambda agent_id, *args, **kwargs: \n",
    "                                                                  player_to_agent[agent_id]))\n",
    "    else:\n",
    "        print(\"DEBUG: Use shared policy\")\n",
    "        base_env = make_envs.env_creator(run_configs.env_config)\n",
    "        policies = {}\n",
    "        rgb_shape = base_env.observation_space[\"player_0\"][\"RGB\"].shape\n",
    "        sprite_x = rgb_shape[0]\n",
    "        sprite_y = rgb_shape[1]\n",
    "\n",
    "        policies[f\"shared_agent\"] = policy.PolicySpec(\n",
    "            observation_space=base_env.observation_space[f\"player_0\"],\n",
    "            action_space=base_env.action_space[f\"player_0\"],\n",
    "            config={\n",
    "                \"model\": {\n",
    "                    \"conv_filters\": [[16, [8, 8], 1],\n",
    "                                    [128, [sprite_x, sprite_y], 1]],\n",
    "                },\n",
    "            })\n",
    "        \n",
    "        run_configs.multi_agent(policies=policies, policy_mapping_fn=(lambda agent_id, *args, **kwargs:\n",
    "                                                                    \"shared_agent\"))\n",
    "        \n",
    "        \n",
    "    \n",
    "    run_configs.model[\"fcnet_hiddens\"] = params_dict['fcnet_hidden']\n",
    "    run_configs.model[\"post_fcnet_hiddens\"] = params_dict['post_fcnet_hidden']\n",
    "    run_configs.model[\"conv_activation\"] = params_dict['cnn_activation'] \n",
    "    run_configs.model[\"fcnet_activation\"] = params_dict['fcnet_activation']\n",
    "    run_configs.model[\"post_fcnet_activation\"] = params_dict['post_fcnet_activation']\n",
    "    run_configs.model[\"use_lstm\"] = params_dict['use_lstm']\n",
    "    run_configs.model[\"max_seq_len\"] = params_dict['max_seq_len']\n",
    "    run_configs.model[\"lstm_use_prev_action\"] = params_dict['lstm_use_prev_action']\n",
    "    run_configs.model[\"lstm_use_prev_reward\"] = params_dict['lstm_use_prev_reward']\n",
    "    run_configs.model[\"lstm_cell_size\"] = params_dict['lstm_cell_size']\n",
    "\n",
    "    # Experiment Trials\n",
    "    experiment_configs['name'] = params_dict['exp_name']\n",
    "    experiment_configs['stop'] = params_dict['stopping']\n",
    "    experiment_configs['keep_checkpoints_num'] = params_dict['num_checkpoints']\n",
    "    experiment_configs['checkpoint_freq'] = params_dict['checkpoint_interval']\n",
    "    experiment_configs['checkpoint_at_end'] = params_dict['checkpoint_at_end']\n",
    "    # experiment_configs['keep'] = params_dict['num_checkpoints']\n",
    "    # experiment_configs['freq'] = params_dict['checkpoint_interval']\n",
    "    # experiment_configs['end'] = params_dict['checkpoint_at_end']\n",
    "    if args.framework == 'tf':\n",
    "        experiment_configs['local_dir'] = f\"{params_dict['results_dir']}/tf\"\n",
    "    else:\n",
    "        experiment_configs['local_dir'] = f\"{params_dict['results_dir']}/torch\"\n",
    " \n",
    "    return run_configs, experiment_configs, tune_configs\n",
    "\n",
    "configs, exp_config, _ = get_experiment_config(args, default_config)\n",
    "# trainer:ppo.PPO = ppo.PPO.from_checkpoint(ckp)\n",
    "# trainer.restore(ckp)\n",
    "# print(f\"Continuing training from {ckp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 15:53:37,058\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2024-05-24 15:53:37,059\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='meltingpot', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('meltingpot').build()` instead. This will raise an error in the future!\n",
      "/nas/wjxie/anaconda3/envs/gtp/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/nas/wjxie/anaconda3/envs/gtp/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/nas/wjxie/anaconda3/envs/gtp/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/nas/wjxie/anaconda3/envs/gtp/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-24 15:53:49,226\tINFO worker_set.py:297 -- Inferred observation/action spaces from remote worker (local worker has no env): {'agent_1': (Dict('COLLECTIVE_REWARD': Box(-inf, inf, (), float64), 'INVENTORY': Box(-inf, inf, (2,), float64), 'READY_TO_SHOOT': Box(-inf, inf, (), float64), 'RGB': Box(0, 255, (5, 5, 3), uint8)), Discrete(8)), 'agent_0': (Dict('COLLECTIVE_REWARD': Box(-inf, inf, (), float64), 'INVENTORY': Box(-inf, inf, (2,), float64), 'READY_TO_SHOOT': Box(-inf, inf, (), float64), 'RGB': Box(0, 255, (5, 5, 3), uint8)), Discrete(8)), '__env__': (Dict('player_0': Dict('COLLECTIVE_REWARD': Box(-inf, inf, (), float64), 'INVENTORY': Box(-inf, inf, (2,), float64), 'READY_TO_SHOOT': Box(-inf, inf, (), float64), 'RGB': Box(0, 255, (5, 5, 3), uint8)), 'player_1': Dict('COLLECTIVE_REWARD': Box(-inf, inf, (), float64), 'INVENTORY': Box(-inf, inf, (2,), float64), 'READY_TO_SHOOT': Box(-inf, inf, (), float64), 'RGB': Box(0, 255, (5, 5, 3), uint8))), Dict('player_0': Discrete(8), 'player_1': Discrete(8)))}\n",
      "2024-05-24 15:53:49,232\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2024-05-24 15:53:49,652\tINFO policy.py:1294 -- Policy (worker=local) running on 1 GPUs.\n",
      "2024-05-24 15:53:49,653\tINFO torch_policy_v2.py:113 -- Found 1 visible cuda devices.\n",
      "/nas/wjxie/anaconda3/envs/gtp/lib/python3.10/site-packages/torch/nn/modules/linear.py:114: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:56.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "2024-05-24 15:53:50,018\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2024-05-24 15:53:50,051\tINFO policy.py:1294 -- Policy (worker=local) running on 1 GPUs.\n",
      "2024-05-24 15:53:50,052\tINFO torch_policy_v2.py:113 -- Found 1 visible cuda devices.\n",
      "2024-05-24 15:53:50,090\tINFO util.py:118 -- Using connectors:\n",
      "2024-05-24 15:53:50,091\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2024-05-24 15:53:50,091\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2024-05-24 15:53:50,092\tINFO util.py:118 -- Using connectors:\n",
      "2024-05-24 15:53:50,092\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2024-05-24 15:53:50,093\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2024-05-24 15:53:50,093\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['agent_1', 'agent_0']>\n",
      "2024-05-24 15:53:50,093\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'agent_0': None, 'agent_1': None}\n",
      "2024-05-24 15:53:50,094\tINFO rollout_worker.py:550 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2024-05-24 15:53:50,123\tINFO trainable.py:172 -- Trainable.setup took 13.060 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-24 15:53:50,124\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# trainer = ray.tune.registry.get_trainable_cls(trainer)\n",
    "trainer_cls = ray.tune.tune._get_trainable(trainer)\n",
    "trainer_obj = trainer_cls(configs, \"meltingpot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO\n"
     ]
    }
   ],
   "source": [
    "print(trainer_obj)\n",
    "# print(dir(trainer_obj))\n",
    "# print(trainer_obj.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_params:  1143598\n",
      "odict_keys(['cnns', 'one_hot', 'flatten', 'flatten_0', 'flatten_1', 'flatten_2', 'cnn_3', 'post_fc_stack', 'lstm', '_logits_branch', '_value_branch'])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ComplexInputNetwork_as_LSTMWrapper(\n",
      "  (cnns): ModuleDict(\n",
      "    (3): VisionNetwork(\n",
      "      (_convs): Sequential(\n",
      "        (0): SlimConv2d(\n",
      "          (_model): Sequential(\n",
      "            (0): ZeroPad2d((3, 4, 3, 4))\n",
      "            (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(1, 1))\n",
      "            (2): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimConv2d(\n",
      "          (_model): Sequential(\n",
      "            (0): Conv2d(16, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "            (1): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (2): Flatten(start_dim=1, end_dim=-1)\n",
      "      )\n",
      "      (_value_branch_separate): Sequential(\n",
      "        (0): SlimConv2d(\n",
      "          (_model): Sequential(\n",
      "            (0): ZeroPad2d((3, 4, 3, 4))\n",
      "            (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(1, 1))\n",
      "            (2): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimConv2d(\n",
      "          (_model): Sequential(\n",
      "            (0): Conv2d(16, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "            (1): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (2): SlimConv2d(\n",
      "          (_model): Sequential(\n",
      "            (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (one_hot): ModuleDict()\n",
      "  (flatten): ModuleDict(\n",
      "    (0): FullyConnectedNetwork(\n",
      "      (_hidden_layers): Sequential(\n",
      "        (0): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_value_branch_separate): Sequential(\n",
      "        (0): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_value_branch): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): FullyConnectedNetwork(\n",
      "      (_hidden_layers): Sequential(\n",
      "        (0): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_value_branch_separate): Sequential(\n",
      "        (0): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_value_branch): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): FullyConnectedNetwork(\n",
      "      (_hidden_layers): Sequential(\n",
      "        (0): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_value_branch_separate): Sequential(\n",
      "        (0): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): SlimFC(\n",
      "          (_model): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (_value_branch): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatten_0): FullyConnectedNetwork(\n",
      "    (_hidden_layers): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch_separate): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatten_1): FullyConnectedNetwork(\n",
      "    (_hidden_layers): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch_separate): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatten_2): FullyConnectedNetwork(\n",
      "    (_hidden_layers): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch_separate): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cnn_3): VisionNetwork(\n",
      "    (_convs): Sequential(\n",
      "      (0): SlimConv2d(\n",
      "        (_model): Sequential(\n",
      "          (0): ZeroPad2d((3, 4, 3, 4))\n",
      "          (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(1, 1))\n",
      "          (2): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimConv2d(\n",
      "        (_model): Sequential(\n",
      "          (0): Conv2d(16, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "          (1): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (_value_branch_separate): Sequential(\n",
      "      (0): SlimConv2d(\n",
      "        (_model): Sequential(\n",
      "          (0): ZeroPad2d((3, 4, 3, 4))\n",
      "          (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(1, 1))\n",
      "          (2): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (1): SlimConv2d(\n",
      "        (_model): Sequential(\n",
      "          (0): Conv2d(16, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "          (1): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (2): SlimConv2d(\n",
      "        (_model): Sequential(\n",
      "          (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_fc_stack): FullyConnectedNetwork(\n",
      "    (_hidden_layers): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch_separate): Sequential(\n",
      "      (0): SlimFC(\n",
      "        (_model): Sequential(\n",
      "          (0): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (_value_branch): SlimFC(\n",
      "      (_model): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(136, 256, batch_first=True)\n",
      "  (_logits_branch): SlimFC(\n",
      "    (_model): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_value_branch): SlimFC(\n",
      "    (_model): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# trainer.training_iteration()\n",
    "worker:ray.rllib.evaluation.rollout_worker.RolloutWorker= trainer_obj.workers.local_worker()\n",
    "policy_spec = worker.policy_dict['agent_0']\n",
    "policy_instance = worker.policy_map['agent_0']\n",
    "nn_model:nn.Module = policy_instance.model \n",
    "# print(dir(worker))\n",
    "# print(isinstance(nn_model,nn.Module))\n",
    "print(\"Total_params: \", sum([param.nelement() for param in nn_model.parameters()]))\n",
    "# print(nn_model.state_dict().keys())\n",
    "print(nn_model._modules.keys())\n",
    "print(\"-\"*100)\n",
    "print(nn_model)\n",
    "# print(\"Total_params: \", nn_model.total_parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
